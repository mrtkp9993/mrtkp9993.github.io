<!DOCTYPE html>
<html>

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A Brief Introduction to Bayesian Statistics</title>
    <!-- HTML Meta Tags -->
    <meta name="description" content="Notes & Articles about math and statistics.">

    <!-- Facebook Meta Tags -->
    <meta property="og:url" content="https://muratkoptur.com/articles_index.html">
    <meta property="og:type" content="website">
    <meta property="og:title" content="Articles - Murat Koptur - Data Scientist">
    <meta property="og:description" content="Notes & Articles about math and statistics.">
    <meta property="og:image" content="https://muratkoptur.com/articles_og.png">

    <!-- Twitter Meta Tags -->
    <meta name="twitter:card" content="summary_large_image">
    <meta property="twitter:domain" content="muratkoptur.com">
    <meta property="twitter:url" content="https://muratkoptur.com/articles_index.html">
    <meta name="twitter:title" content="Articles - Murat Koptur - Data Scientist">
    <meta name="twitter:description" content="Notes & Articles about math and statistics.">
    <meta name="twitter:image" content="https://muratkoptur.com/articles_og.png">

    <!-- Meta Tags Generated via https://www.opengraph.xyz -->

    <link rel="stylesheet" href="https://latex.now.sh/style.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body class="libertinus">
    <header>
        <h1>A Brief Introduction to Bayesian Statistics</h1>
        <p class="author">Murat Koptur <br> VERSION 1.0 - 24/03/2023</p>
    </header>

    <div class="abstract">
        <blockquote>
            Achieving replicability is important for making research progress. If findings are not replicable, then
            prediction and theory development are stifled. If findings are replicable, then interrogation of their
            meaning
            and validity can
            advance knowledge. Assessing replicability can be productive for generating and testing hypotheses by
            actively
            confronting current understandings to identify weaknesses and spur innovation.
            <cite>â€” Brian Nosek</cite>
        </blockquote>
    </div>

    <main>
        <article class="indent-pars">
            <h2>Fundamental Definitions</h2>
            <p>
                Probability theory is a mathematical framework that allows us to reason about experiments whose outcome
                is
                uncertain. A probabilistic model is a mathematical model of a probabilistic experiment that satisfies
                certain
                mathematical properties (the axioms of probability theory), and which allows us to calculate
                probabilities
                and
                to reason about the likely outcomes of the experiment. <label for="sn-1"
                    class="sidenote-toggle sidenote-number"></label>
                <input type="checkbox" id="sn-1" class="sidenote-toggle" />
                <span class="sidenote">Source:
                    <a
                        href="https://ocw.mit.edu/courses/6-436j-fundamentals-of-probability-fall-2018/c37dc8b61cdf6bde689a627bfa5b4942_MIT6_436JF18_lec01.pdf">
                        Link
                    </a>
                </span>
            </p>
            <div class="definition">
                A probabilistic model is defined by a triple $(\Omega, \mathcal{F}, \mathbb{P}) $, called a probability
                space, consists of the following three elements:
                <ol>
                    <li>$\Omega $ is the sample space, the set of possible experiment outcomes.</li>
                    <li>$\mathcal{F} $ is a $\sigma $-algebra, a collection of subsets of $\Omega $.</li>
                    <li>$\mathbb{P} $ is a probability measure, a function that assigns a nonnegative probability to
                        every
                        set in the $\sigma $-algebra $\mathcal{F} $. <label for="sn-2"
                            class="sidenote-toggle sidenote-number"></label>
                        <input type="checkbox" id="sn-2" class="sidenote-toggle" />
                        <span class="sidenote">Source:
                            <a
                                href="https://ocw.mit.edu/courses/6-436j-fundamentals-of-probability-fall-2018/c37dc8b61cdf6bde689a627bfa5b4942_MIT6_436JF18_lec01.pdf">
                                Link
                            </a>
                        </span>
            </div>
            <p>
                We'll not give measure-theoretical introduction here. For more measure-theoretical introduction to
                probability
                theory, please read <sup>2</sup>.
            </p>
            <div class="definition">Let $A $ and $B $ be events in discrete sample space $S, $ and $p(B)\neq0 $.
                Then Bayes' theorem is stated as
                $$
                p(A|B)=\frac{p(B|A)p(A)}{p(B)}.
                $$
                In this formula
                <ul>
                    <li>
                        $p(A|B) $ is the probability of event $A $ occuring given that $B $ is true. It is also called
                        the
                        posterior probability of $A $ given $B $.
                    </li>
                    $p(B|A) $ is the probability of event $B $ occurring given that $A $ is true. It can be also be
                    interpreted as the likelihood of $A $ given a fixed $B $.
                    </li>
                    <li>$p(A) $ and $p(B) $ are the probabilities of observing $A $ and $B $ respectively without any
                        conditions; they are known as the prior probability and marginal probability.
                        <label for="sn-3" class="sidenote-toggle sidenote-number"></label>
                        <input type="checkbox" id="sn-3" class="sidenote-toggle" />
                        <span class="sidenote">Source: <a
                                href="https://www.wikiwand.com/en/Bayes%27_theorem">Link</a></span>
                    </li>
                </ul>

            </div>
            <p>
                Suppose that we have a medical test with following probabilities:
            <ul>
                <li>Disease occuring rate in a general population is $1\% $,</li>
                <li>If you have the disease, the test shows that you do with probability $80\% $, </li>
                <li>If you don't have the disease, it shows that you do with probability $9.6\% $. </li>
            </ul>
            The question is, how likely is that you have the disease if the test is positive?</p>
            <p>
                Let's use following notation: $H $ is being healthy, $S $ is being sick, $+ $ is positive test result,
                and
                $- $ is a negative test result.
                Then, this probability can be calculated
                as
                $$
                \begin{aligned} p(S|+) &= \frac{p(+|S)p(S)}{p(+)} \\
                &= \frac{0.8\times 0.01}{0.01\times0.8 + 0.99\times0.096} \\
                &= 0.077 \end{aligned}
                $$

                So, the probability of being sick is $7.8\%$.
                Let's think the doctor wants to re-test the patient to be sure. If we update the probability of being
                sick with $7.8\%$, then the probability of being sick if the test is positive becomes

                $$
                p(S|+) = \frac{0.8\times 0.077}{0.077\times0.8 + 0.923\times0.096} \approx 0.41
                $$

                After the second test, if second test is positive, the probability of being sick is nearly $41\%$.
                This example shows the power of Bayes' theorem, it allows to update our beliefs when our data is
                updated.
            </p>
            <p>
                There is another way to interpret Bayes' theorem: Diachronic interpretation.
                Suppose that $H $ is our hypothesis and $D $ is our data. Then Bayes' theorem can be written as
                $$
                p(H|D) = \frac{p(D|H)p(H)}{p(D)}.
                $$
                Here is <label for="sn-4" class="sidenote-toggle sidenote-number"></label>
                <input type="checkbox" id="sn-4" class="sidenote-toggle" />
                <span class="sidenote">Source: <a
                        href="https://www.researchgate.net/publication/338896961_Bayesci_Istatistige_Giris">Link</a></span>:
            <ul>
                <li>
                    $p(H) $ is the prior probability which is probability of hypothesis $H $ before data $D $ is
                    observed / collected.
                </li>
                <li>
                    $p(D|H) $ is the probability of the data under the hypothesis, called the likelihood.
                </li>
                <li>
                    $p(D) $ is the probability of the data under any hypothesis, called the normalizing constant. This
                    constant amkes the posterios density integrate to one.
                </li>
                <li>
                    $p(H|D) $ is the probability of the hypothesis after data is collected, called the posterior
                    probability.
                </li>
            </ul>
            In practice, $p(D) $ is difficult to calculate in a closed-form, so one often uses the following adaptation
            of
            Bayes' formula
            $$
            p(H|D) \propto p(D|H)p(H)
            $$
            </p>
            <p>
                As a second example, let's consider the famous Monty Hall problem. The problem is stated as follows in
                Parade
                magazine in 1990:
            <blockquote>
                Suppose you're on a game show, and you're given the choice of three doors: Behind one door is a car;
                behind
                the others,
                goats. You pick a door, say No. 1, and the host, who knows what's behind the doors, opens another door,
                say
                No. 3, which
                has a goat. He then says to you, "Do you want to pick door No. 2?" Is it to your advantage to switch
                your
                choice?<label for="sn-5" class="sidenote-toggle sidenote-number"></label>
                <input type="checkbox" id="sn-5" class="sidenote-toggle" />
                <span class="sidenote">Source: <a href="https://www.wikiwand.com/en/Monty_Hall_problem">Link</a></span>
            </blockquote>
            </p>
            <p>
                Let's look at the probability table of the problem:
            <table>
                <thead>
                    <tr>
                        <th>Door 1</th>
                        <th>Door 2</th>
                        <th>Door 3</th>
                        <th>Result if not switch doors</th>
                        <th>Result if switch doors</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Car</td>
                        <td>Goat</td>
                        <td>Goat</td>
                        <td>Wins Car</td>
                        <td>Losts Car</td>
                    </tr>
                    <tr>
                        <td>Goat</td>
                        <td>Car</td>
                        <td>Goat</td>
                        <td>Losts Car</td>
                        <td>Wins Car</td>
                    </tr>
                    <tr>
                        <td>Goat</td>
                        <td>Goat</td>
                        <td>Car</td>
                        <td>Losts Car</td>
                        <td>Wins Car</td>
                    </tr>
                </tbody>
            </table>
            <br>
            If you switch the door, you will win the car with probability $2/3$, so the switching strategy is a better
            strategy than not switching the doors.
            </p>
            <p>Let's examine the problem with Bayes' theorem now. Assume that you choose Door 1, and the host opens Door
                2.
                Then:
            <ul>
                <li>If the car is behind Door 1, the host can open Door 2 or Door 3.
                    So
                    $$
                    p(A|D)=\frac{\frac{1}{2}\times\frac{1}{3}}{\frac{1}{2}\times\frac{1}{3}+0\times\frac{1}{3}+1*\frac{1}{3}}=\frac{1}{3}.
                    $$
                </li>
                <li>If the car is behind Door 2, the host will not open Door 2. So $$
                    p(B|D)=\frac{0\times\frac{1}{3}}{\frac{1}{2}\times\frac{1}{3}+0\times\frac{1}{3}+1*\frac{1}{3}}=0.
                    $$
                </li>
                <li>If the car is behind Door 3, the host will open Door 2 all the time. So
                    $$
                    p(C|D)=\frac{1\times\frac{1}{3}}{\frac{1}{2}\times\frac{1}{3}+0\times\frac{1}{3}+1*\frac{1}{3}}=\frac{1}{3}.
                    $$
                </li>
            </ul>
            If we calculate the posterior probability for each hypothesis, we have the following results:
            <table>
                <thead>
                    <tr>
                        <th>Hypothesis</th>
                        <th>Prior probability $p(H) $</th>
                        <th>Likelihood $p(D|H) $</th>
                        <th>Posterior probability $p(H|D) $</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>A</td>
                        <td>1/3</td>
                        <td>1/2</td>
                        <td>1/3</td>
                    </tr>
                    <tr>
                        <td>B</td>
                        <td>1/3</td>
                        <td>0</td>
                        <td>0</td>
                    </tr>
                    <tr>
                        <td>C</td>
                        <td>1/3</td>
                        <td>1</td>
                        <td>2/3</td>
                    </tr>
                </tbody>
            </table>
            </p>

            <h2>Differences between Frequentist and Bayesian Statistics</h2>
            <p>
                Regardless of which statistical approach, any statistical inference paradigm deals with three things:
            <ul>
                <li>The quantities which we want to estimate or test, called parameters.</li>
                <li>Observed and/or collected data.</li>
                <li>Models which help us to relate data and parameters.</li>
            </ul>
            In the frequentist view of probability:
            <ul>
                <li>Data is random because they are observations of stochastic processes.</li>
                <li>Model parameters are fixed because they remain constant during the sampling process.</li>
            </ul>
            In Bayesian view of probability:
            <ul>
                <li>Data is fixed.</li>
                <li>Model parameters are treated and described probabilistically. And they are defined via probability
                    distributions.</li>
            </ul>
            </p>

            <h2>Bayesian Inference</h2>
            <p>
                Bayesian inference can be described in three steps:
                <label for="sn-6" class="sidenote-toggle sidenote-number"></label>
                <input type="checkbox" id="sn-6" class="sidenote-toggle" />
                <span class="sidenote">Source: A. Gelman, J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari,D. B.
                    Rubin, Bayesian data analysis, third edition, 2013.</span>
            <ol>
                <li>Setting up full probability model: For all observed and unobserved quantities, a joint probability
                    distribution must be defined.
                    <br>
                    Suppose $y $ is observed data and $\theta $ is parameters. Then joint probability distribution is
                    defined as
                    $$
                    p(\theta, y) = p(y|\theta)p(\theta).
                    $$
                    Here, $p(\theta) $ is prior distribution and $p(y|\theta) $ is the sampling distribution.
                </li>
                <li>
                    Calculating the posterior distribution: After the data were observed, posterior distribution will be
                    calculated with Bayes' theorem:
                    $$
                    p(\theta|y)=\frac{p(\theta)p(y|\theta)}{p(y)}\propto p(\theta)p(y|\theta)
                    $$
                </li>
                <li>
                    Model diagnostics: How well does the model fit the data? How sensitive are the results to the
                    modeling assumptions in step 1? We will dive into tests a few chapters later.
                </li>
            </ol>

            Posterior distribution has all the information about our parameters. But, in practice, we need some tools to
            summarize
            information. Graphically, if our parameter vector has low dimension (1-2), we can plot the distribution. If
            our parameter vector has higher
            dimension, we can plot marginal posterior distributions.

            Numerically, we can use point estimation and interval estimation methods. For point estimations, we have
            different options:
            <ul>
                <li>Posterior mode
                    $$
                    \hat{\theta}=\max_\theta[p(\theta|y)]
                    $$
                </li>
                <li>
                    Posterior mean
                    $$
                    \hat{\theta}=\mathbb{E}[\theta|y]=\int{y p(\theta|y) d\theta}
                    $$
                </li>
            </ul>
            For interval estimations, we can use:
            <ul>
                <li>(Credible interval) A $100(1-\alpha)\%$ credible interval for parameter $\theta $ is an interval
                    $[a, b] $ such that the probability that $\theta $ lies in the interval is $1-\alpha$<label
                        for="sn-7" class="sidenote-toggle sidenote-number"></label>
                    <input type="checkbox" id="sn-7" class="sidenote-toggle" />
                    <span class="sidenote">Source: <a
                            href="https://web.stanford.edu/class/archive/cs/cs109/cs109.1218/files/student_drive/8.2.pdf">Link</a></span>:
                    $$
                    p(\theta\in[a,b])=1-\alpha
                    $$
                    If parameter vector has a dimension bigger than one, this is called as credible region.
                </li>
                <li>Highest density region is the region with $100(1-\alpha)\%$ probability of containing $\theta$ with
                    the shortest total length.</li>
            </ul>
            </p>

            <h2>Prior Distribution Recommendations</h2>
            <p>Stan (Bayesian inference package) has a great <a
                    href="https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations">wiki</a> about selecting
                prior distributions for analyses.</p> Historically, due to lack of necessary computation power and
            methods,
            conjugate priors were mostly selected for ease of calculation of the posterior. Today with faster computers
            and
            MCMC sampling methods, nearly any arbitrary posterior can be sampled.
            </p>

            <h2>Model Diagnostics</h2>
            <p>
                Due to nature of MCMC algorithms, we need to make some posterior diagnostic tests. These tests are:
            <ul>
                <li>Checking traceplots: Sampling was done with multiple chains and and all chains are expected to have
                    similar results.
                </li>
                <li>Checking convergence: All chains must converge.</li>
                <li>Effective sample size (ESS): Suppose $m$ is the chain count, $n $ is length of a chain,
                    $\hat{\rho_t}$ is estimated autocorrelation at lag $t $ and $T $ is the first positive odd number
                    which makes
                    $\hat{\rho_{T+1}}+\hat{\rho_{T+2}}$ sum is negative. Then effective sample size is defined as

                    $$
                    \hat{n}_\text{eff}=\frac{mn}{1+2\sum_{t=1}^T\hat{\rho}_t}.
                    $$
                    In practice, it is expected to be $\frac{\hat{n}_\text{eff}}{mn}>0.001.$
                </li>
                <li>Gelman-Rubin statistic: It is defined as ratio of between-chain variance to within-chain variance.
                    We should run our chains until this value is close to 1.
                </li>
            </ul>
            </p>

            <h2>Information Criterions</h2>
            <p>
                To compare and select the best model for our data, we need to score models. To compare Bayesian models,
                following statistics can be used:
            <ul>
                <li>Within-sample predictive accuracy

                    $$
                    \text{lppd}=\sum_{i=1}^N \log\int{p(y_i|\theta)p_{\text{post}}(\theta)}d\theta\approx\sum_{i=1}^N
                    \log(\frac{1}{M}\sum_{j=1}^M p(y_j|\theta^{(j)}))
                    $$
                </li>
                <li>
                    Widely-Applicable Information Criterion (WAIC)

                    $$
                    \text{WAIC}=-2\text{lppd} + p_{\text{WAIC}}
                    $$

                    where

                    $$
                    p_{\text{WAIC}}=2\sum_{i=1}^N[\log(\frac{1}{M}\sum_{j=1}^M
                    p(y_j|\theta^{(j)}))-\frac{1}{M}\sum_{j=1}^M p(y_j|\theta^{(j)})]
                    $$
                </li>
                <li>
                    Leave-One-Out Cross-Validation (LOO)

                    $$
                    \text{loo} = \sum_{i=1}^n \log p(y_i|y_{-i})
                    $$

                    where

                    $$
                    p(y_i|y_{-i})=\int p(y_i|\theta)p(\theta|y_{-i})d\theta
                    $$

                    is the leave-one-out predictive density given the data without the $i-$th data point.
                </li>
            </ul>
            </p>

            <h2>Hypothesis Tests & Significance Tests</h2>
            <p>Suppose that $H_0 $ is the null hypothesis and $H_1 $ is the alternative hypothesis. From Bayes' theorem,
                we can calculate corresponding posterior distributions, $p(H_0|D)=\frac{p(D|H_0)p(H_0)}{p(D)}$ and
                $p(H_1|D)=\frac{p(D|H_1)p(H_1)}{p(D)}$, respectively. We can calculate the likelihood ratio

                $$
                \frac{p(H_0|D)}{p(H_1|D)} = \frac{p(D|H_0)}{p(D|H_1)}\frac{p(H_0)}{p(H_1)}
                $$

                In this expression, the $\frac{p(D|H_0)}{p(D|H_1)}$ ratio is called Bayes factor. In the table below,
                you can
                find interpretations of Bayes factors.

            <table>
                <thead>
                    <tr>
                        <th>$\log_{10}(\text{Bayes Factor})$</th>
                        <th>Bayes Factor</th>
                        <th>Evidence (for $H_1$)
                        </th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>[-Inf,0[</td>
                        <td>[0,1[</td>
                        <td>Negative</td>
                    </tr>
                    <tr>
                        <td>[0, 0.5[</td>
                        <td>[1, 3.2[</td>
                        <td>Weak</td>
                    </tr>
                    <tr>
                        <td>[0.5, 1[</td>
                        <td>[3.2, 10[</td>
                        <td>Substantial</td>
                    </tr>
                    <tr>
                        <td>[1, 1.5[</td>
                        <td>[10, 32[</td>
                        <td>Strong</td>
                    </tr>
                    <tr>
                        <td>[1.5, 2[</td>
                        <td>[32, 100[</td>
                        <td>Very Strong</td>
                    </tr>
                    <tr>
                        <td>[2, +Inf[</td>
                        <td>[100, +Inf[</td>
                        <td>Decisive</td>
                    </tr>
                </tbody>
            </table>
            <br>
            For testing parameter significance, we can use region of practical equivalence (ROPE). In this method,
            The null value is declared to be rejected if the (89% or 95%) HDI falls completely outside the ROPE, and the
            null value is
            declared to be accepted if the 89%/95% HDI falls completely inside the ROPE. But how do we set the limits of
            the ROPE? How big is "practically equivalent to the null value"? Since it depends on the particular
            application domain and current
            best practices there, there is typically no one right answer to this question. <label for="sn-8"
                class="sidenote-toggle sidenote-number"></label>
            <input type="checkbox" id="sn-8" class="sidenote-toggle" />
            <span class="sidenote">Source: <a
                    href="http://doingbayesiandataanalysis.blogspot.com/2013/08/how-much-of-bayesian-posterior.html">Link</a></span>
            </p>

            <h2>Bayesian Regression Models</h2>
            <p>
                Linear regression is basis of many models in statistical analysis. So we will start with it. A linear
                regression model
                with one dependent and one independent variable can be written as
                $$
                y=\hat{\beta}_0 + \hat{\beta}_1 x + \epsilon, \quad \epsilon\sim\text{Normal}(0,\sigma^2)
                $$
                Here, $\epsilon=y-\hat{y} $ is residuals. In Bayesian linear regression, prior distributions for
                intercepts and coefficients can be selected as Normal / Student-T
                distributions.
            </p>
            <p>
                ANOVA models can be seen as linear regression models where the independent variables are dummy
                variables. Similarly,
                ANCOVA models can be seen as linear regression models where the independent variables can be dummy and
                continuous. For Bayesian ANOVA and ANCOVA models, R2 family can be used as prior distributions.<label
                    for="sn-9" class="sidenote-toggle sidenote-number"></label>
                <input type="checkbox" id="sn-9" class="sidenote-toggle" />
                <span class="sidenote">Source: <a
                        href="http://mc-stan.org/rstanarm/reference/priors.html">Link</a></span>
            </p>
            <p>
                Linear mixed effect models are used to analyze grouped or hierarchical data. These models can be
                classified as follows<label for="sn-10" class="sidenote-toggle sidenote-number"></label>
                <input type="checkbox" id="sn-10" class="sidenote-toggle" />
                <span class="sidenote">Source: F. Korner-Nievergelt, T. Roth, S. von Felten, J. GuÃ©lat, B. Al-masi, P.
                    Korner-Nievergelt, Bayesian Data Analysis in
                    Ecology Using Linear Models with R, BUGS, and Stan, 2015.
                </span>:
            <ul>
                <li>
                    Complete pooling: Group structure is omitted.
                    $$
                    \begin{aligned}
                    \hat{y}_i &= \beta_0 \\
                    y_i&\sim \text{Normal}(\hat{y}_i, \sigma^2)
                    \end{aligned}
                    $$
                </li>
                <li>
                    Partial pooling: Group means are weighted averages of the population mean and the unpooled group
                    means.
                    $$
                    \begin{aligned}
                    \hat{y}_i &= \beta_0 + b_{g_i} \\
                    y_i&\sim \text{Normal}(\hat{y}_i, \sigma^2) \\
                    b_g&\sim \text{Normal}(0, \sigma_b^2)
                    \end{aligned}
                    $$
                </li>
                <li>
                    No pooling: Group means can be estimated separately for
                    each group.
                    $$
                    \begin{aligned}
                    \hat{y}_i &= \beta_{g_i} \\
                    y_i&\sim \text{Normal}(\hat{y}_i, \sigma_{g_i}^2)
                    \end{aligned}
                    $$
                </li>
                Hyperprior or multivariate distributions can be used as prior distributions of parameters of these
                models.
            </ul>
            </p>
            <p>
                What if our dependent variables are not continuous but binary variables, ratio variables, or count
                variables?
                In this case, generalized linear models can be used to model our variables. Generalized linear models
                consists of three elements<label for="sn-11" class="sidenote-toggle sidenote-number"></label>
                <input type="checkbox" id="sn-11" class="sidenote-toggle" />
                <span class="sidenote">Source: <a
                        href="https://www.wikiwand.com/en/Generalized_linear_model">Link</a></span>:
            <ul>
                <li>
                    A distribution for modelling dependent variable $Y$,
                </li>
                <li>A linear predictor $\eta=X\beta$,</li>
                <li>A link function $g $ such that $\mathbb{E}(Y|X)=\mu=g^{-1}(\eta) $.</li>
            </ul>
            If a generalized linear model contains both fixed and random effects, it is called generalized linear mixed
            model. In
            Bayesian modeling, priors for parameters of generalized linear models / generalized linear mixed models can
            be selected
            similarly to linear models / linear mixed effect models.
            </p>
            <p>
                This text only examines models for cross-sectional data. Time-series and longitudinal data models will
                not be explained
                here.
            </p>
            <hr>
            <p>
                Thanks for reading! I hope you found this article informative and insightful. If you have any questions
                or feedback,
                please don't hesitate to reach out to me. I always appreciate hearing from my readers and learning how I
                can improve my
                writing. If you notice any errors or inaccuracies, please notify me, and I'll make the necessary
                corrections.

                How to contact me: <a href="mailto:muratkoptur@yandex.com">E-mail</a> <a
                    href="https://www.linkedin.com/in/muratkoptur/">LinkedIn</a>
            </p>
        </article>
    </main>

</body>

</html>